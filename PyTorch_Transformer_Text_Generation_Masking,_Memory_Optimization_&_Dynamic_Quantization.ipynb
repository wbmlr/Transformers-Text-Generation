{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/torch/text/datasets/IMDB"
      ],
      "metadata": {
        "id": "eo0fLXwbWEOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchtext -y\n",
        "!pip install torch torchtext --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install 'numpy<2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OyfAJ07pO18h",
        "outputId": "aca25e18-6013-4d89-f4d8-82b8bb955065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.0+cu118\n",
            "Uninstalling torch-2.2.0+cu118:\n",
            "  Successfully uninstalled torch-2.2.0+cu118\n",
            "Found existing installation: torchtext 0.17.0+cpu\n",
            "Uninstalling torchtext-0.17.0+cpu:\n",
            "  Successfully uninstalled torchtext-0.17.0+cpu\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchtext\n",
            "  Using cached https://download.pytorch.org/whl/torchtext-0.17.0%2Bcpu-cp311-cp311-linux_x86_64.whl (2.0 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Collecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (811.7 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1->torchtext) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch, torchtext\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0+cu118 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.0+cu118 torchtext-0.17.0+cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchtext"
                ]
              },
              "id": "47889bf91f554e4cbb1f12bbfd69663b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 362, in run\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8GUTEw-L0gm"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F\n",
        "import torch.quantization\n",
        "import math # For positional encoding\n",
        "from torch.cuda.amp import autocast, GradScaler # For mixed precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixDcypebOIjh",
        "outputId": "926ea5b6-5fad-4a51-fd47-2bfeb5ab7d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-8c2d8b7a0130>\", line 1, in <cell line: 0>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 1471, in <module>\n",
            "    from .functional import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/functional.py\", line 9, in <module>\n",
            "    import torch.nn.functional as F\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/__init__.py\", line 1, in <module>\n",
            "    from .modules import *  # noqa: F403\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
            "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
            "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup Global Variables and Special Tokens ---\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "metadata": {
        "id": "TVnKaphHOIgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Data Loading and Preprocessing ---\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
        "                                  min_freq=1,\n",
        "                                  specials=special_tokens,\n",
        "                                  special_first=True)\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "def text_pipeline(text):\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 128 # Reduced for memory efficiency\n",
        "\n",
        "def collate_batch(batch):\n",
        "    data = []\n",
        "    for _, text in batch:\n",
        "        token_ids = [BOS_IDX] + text_pipeline(text) + [EOS_IDX]\n",
        "        if len(token_ids) > MAX_SEQUENCE_LENGTH:\n",
        "            token_ids = token_ids[:MAX_SEQUENCE_LENGTH - 1] + [EOS_IDX]\n",
        "        data.append(torch.tensor(token_ids, dtype=torch.long))\n",
        "\n",
        "    data = pad_sequence(data, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # For Transformer: input `src` (tokens up to t-1), target `tgt` (tokens from t)\n",
        "    # Plus padding mask for `src`\n",
        "    src = data[:, :-1]\n",
        "    tgt = data[:, 1:]\n",
        "\n",
        "    # src_padding_mask: True where PAD_IDX\n",
        "    src_padding_mask = (src == PAD_IDX) # Boolean mask: (batch_size, seq_len)\n",
        "\n",
        "    return src, tgt, src_padding_mask # Return input, target, and padding mask"
      ],
      "metadata": {
        "id": "ELPQbCmGOIfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper for Causal Mask (needed for autoregressive Transformer) ---\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with 0s on diag & lower.\"\"\"\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask"
      ],
      "metadata": {
        "id": "nfp1wGhtOIcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Positional Encoding (common in Transformers) ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Adjust PE shape to match batch_first (x.size(1) is seq_len)\n",
        "        x = x + self.pe[:x.size(1)].transpose(0,1)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "gtGHfayVOIaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Model Definition (Transformer Text Generator) ---\n",
        "class TransformerTextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, nhead, num_layers, dim_feedforward, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n",
        "\n",
        "        # Transformer Decoder Layer: building block\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=nhead,\n",
        "                                                 dim_feedforward=dim_feedforward, dropout=dropout,\n",
        "                                                 batch_first=True)\n",
        "        # Transformer Decoder: stacks multiple decoder layers\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "        self.init_weights()\n",
        "        self.nhead = nhead # Store nhead for later use if needed (e.g. for generation hidden state handling)\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask, src_padding_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape (batch_size, seq_len)\n",
        "            src_mask: Tensor, shape (seq_len, seq_len) for causal masking\n",
        "            src_padding_mask: Tensor, shape (batch_size, seq_len) for padding\n",
        "        \"\"\"\n",
        "        src = self.embedding(src) * math.sqrt(self.embedding.embedding_dim) # Scale embedding\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        # Transformer Decoder takes:\n",
        "        # tgt: target sequence (here, same as src for language modeling)\n",
        "        # memory: encoder output (not used directly here, but memory_mask needed)\n",
        "        # tgt_mask: causal mask (prevents attending to future tokens)\n",
        "        # memory_mask: mask for memory (not used here, but for consistency set same as tgt_mask)\n",
        "        # tgt_key_padding_mask: mask for padding in tgt\n",
        "        # memory_key_padding_mask: mask for padding in memory (not used here)\n",
        "        output = self.transformer_decoder(tgt=src, memory=src, # For decoder-only LM, tgt and memory are the same\n",
        "                                          tgt_mask=src_mask,\n",
        "                                          memory_mask=src_mask,\n",
        "                                          tgt_key_padding_mask=src_padding_mask,\n",
        "                                          memory_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        output = self.fc(output) # Map to vocabulary logits\n",
        "        return output\n",
        "\n",
        "# Model parameters\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 64\n",
        "NHEAD = 8 # Number of attention heads\n",
        "NUM_LAYERS = 3 # Number of transformer decoder layers\n",
        "DIM_FEEDFORWARD = 128 # Hidden dimension of the feedforward network in each transformer layer\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = TransformerTextGenerator(VOCAB_SIZE, EMBED_DIM, NHEAD, NUM_LAYERS, DIM_FEEDFORWARD, DROPOUT).to(device)"
      ],
      "metadata": {
        "id": "s-MRJRkLOIYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Training Setup ---\n",
        "BATCH_SIZE = 64 # Keep low due to memory\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005) # Slightly lower LR often helpful for Transformers\n",
        "num_epochs = 2 # Increase for better performance\n",
        "\n",
        "# For Mixed Precision Training\n",
        "scaler = GradScaler() # Initialize scaler once"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Y4FquAOIV5",
        "outputId": "0f5dd941-a320-4ec0-8bf2-a7a66bb58e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Training Loop ---\n",
        "def train(dataloader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    for idx, (data, targets, src_padding_mask) in enumerate(dataloader): # Unpack new return values\n",
        "        data, targets, src_padding_mask = data.to(device), targets.to(device), src_padding_mask.to(device)\n",
        "\n",
        "        # Create causal mask for the current sequence length\n",
        "        src_mask = generate_square_subsequent_mask(data.size(1)).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            # Pass all masks to the model\n",
        "            predicted_logits = model(data, src_mask, src_padding_mask)\n",
        "            # Reshape logits for CrossEntropyLoss: (N, C) and (N)\n",
        "            loss = criterion(predicted_logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # Clip gradients to prevent exploding gradients\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        del data, targets, predicted_logits, src_mask, src_padding_mask\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1\n",
        "\n",
        "        if idx % 100 == 0 and idx > 0:\n",
        "            print(f'Epoch {epoch}, Batch {idx}: Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch}: Train Loss: {total_loss/total_batches:.4f}')\n",
        "\n",
        "def evaluate(dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    model_device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (data, targets, src_padding_mask) in enumerate(dataloader):\n",
        "            data, targets, src_padding_mask = data.to(model_device), targets.to(model_device), src_padding_mask.to(model_device)\n",
        "            src_mask = generate_square_subsequent_mask(data.size(1)).to(model_device)\n",
        "\n",
        "            predicted_logits = model(data, src_mask, src_padding_mask)\n",
        "\n",
        "            # --- CHANGE THIS LINE ---\n",
        "            loss = criterion(predicted_logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "            del data, targets, predicted_logits, src_mask, src_padding_mask\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return avg_loss, perplexity"
      ],
      "metadata": {
        "id": "byYk3ISWOIAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/torch/text/datasets/IMDB"
      ],
      "metadata": {
        "id": "3gckR5ocXlBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(train_dataloader, model, criterion, optimizer, epoch)\n",
        "    val_loss = evaluate(test_dataloader, model, criterion)\n",
        "    print(f'Epoch {epoch}: Test Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1ybrH_hOo9R",
        "outputId": "36f4fc9d-bf19-489f-c23c-b06c931299da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss: 9.5896\n",
            "Epoch 1, Batch 200: Loss: 7.6290\n",
            "Epoch 1, Batch 300: Loss: 7.0269\n",
            "Epoch 1: Train Loss: 8.3399\n",
            "Epoch 1: Test Loss: 6.6089\n",
            "Epoch 2, Batch 100: Loss: 6.5115\n",
            "Epoch 2, Batch 200: Loss: 6.1716\n",
            "Epoch 2, Batch 300: Loss: 6.1308\n",
            "Epoch 2: Train Loss: 6.3033\n",
            "Epoch 2: Test Loss: 6.0714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y731xc9VajCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Quantization (Post-Training Dynamic) ---\n",
        "print(\"\\nApplying quantization...\")\n",
        "\n",
        "# Create a copy of the model for quantization\n",
        "# Use the same parameters to create the model_to_quantize\n",
        "model_to_quantize = TransformerTextGenerator(VOCAB_SIZE, EMBED_DIM, NHEAD, NUM_LAYERS, DIM_FEEDFORWARD, DROPOUT)\n",
        "model_to_quantize.load_state_dict(model.state_dict())\n",
        "model_to_quantize.eval() # Set to eval mode for quantization\n",
        "\n",
        "# Apply dynamic quantization to Linear layers and Transformer specific modules\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_to_quantize, {nn.Linear}, dtype=torch.qint8 # <--- ONLY QUANTIZE NN.LINEAR\n",
        ")\n",
        "print(\"Quantization complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpJB64kyOo6d",
        "outputId": "bea22fd3-c9fe-4caf-8366-94097c7dbec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying quantization...\n",
            "Quantization complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Original Model Statistics ---\")\n",
        "# Print model architecture\n",
        "print(model)\n",
        "# Count parameters\n",
        "original_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {original_total_params:,}\")\n",
        "# Calculate in-memory size\n",
        "original_size_mb = sum(p.element_size() * p.numel() for p in model.parameters()) / (1024**2)\n",
        "print(f\"In-memory size: {original_size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Quantized Model Statistics ---\")\n",
        "# Print quantized model architecture (note the 'Quantized' prefix on some layers)\n",
        "print(quantized_model)\n",
        "# Count parameters (note: quantized parameters might report different sizes or types)\n",
        "# For accurate comparison, it's better to look at memory footprint after quantization.\n",
        "quantized_total_params = sum(p.numel() for p in quantized_model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters (quantized): {quantized_total_params:,}\")\n",
        "# Calculate in-memory size (will reflect reduced precision for quantized layers)\n",
        "quantized_size_mb = sum(p.element_size() * p.numel() for p in quantized_model.parameters()) / (1024**2)\n",
        "print(f\"In-memory size: {quantized_size_mb:.2f} MB\")\n",
        "\n",
        "print(f\"\\nMemory Reduction: {((original_size_mb - quantized_size_mb) / original_size_mb) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdHSzwU4bC9_",
        "outputId": "95e13a5c-b0d2-40dd-e1cc-9f3fbecd790d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Original Model Statistics ---\n",
            "TransformerTextGenerator(\n",
            "  (embedding): Embedding(100686, 64, padding_idx=1)\n",
            "  (pos_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer_decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-2): 3 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=64, out_features=100686, bias=True)\n",
            ")\n",
            "Total trainable parameters: 13,139,214\n",
            "In-memory size: 50.12 MB\n",
            "\n",
            "--- Quantized Model Statistics ---\n",
            "TransformerTextGenerator(\n",
            "  (embedding): Embedding(100686, 64, padding_idx=1)\n",
            "  (pos_encoder): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer_decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-2): 3 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (linear1): DynamicQuantizedLinear(in_features=64, out_features=128, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): DynamicQuantizedLinear(in_features=128, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): DynamicQuantizedLinear(in_features=64, out_features=100686, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n",
            "Total trainable parameters (quantized): 6,544,896\n",
            "In-memory size: 24.97 MB\n",
            "\n",
            "Memory Reduction: 50.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Text Generation Example (Adapted for Transformer) ---\n",
        "print(\"\\nExample text generation:\")\n",
        "\n",
        "def generate_text(model, vocab, start_text, max_len=50, temperature=0.8):\n",
        "    model.eval()\n",
        "    input_ids = [BOS_IDX] + text_pipeline(start_text)\n",
        "    generated_ids = list(input_ids)\n",
        "\n",
        "    model_device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            current_sequence_tensor = torch.tensor([generated_ids], dtype=torch.long).to(model_device)\n",
        "\n",
        "            src_mask = generate_square_subsequent_mask(current_sequence_tensor.size(1)).to(model_device)\n",
        "\n",
        "            # --- ADD THIS LINE ---\n",
        "            src_padding_mask = (current_sequence_tensor == PAD_IDX).to(model_device)\n",
        "            # This mask is usually all False for single sequence generation (no padding within it)\n",
        "\n",
        "            output_logits = model(current_sequence_tensor, src_mask, src_padding_mask) # <--- Pass src_padding_mask here\n",
        "\n",
        "            prediction_logits = output_logits[:, -1, :] / temperature\n",
        "            probabilities = F.softmax(prediction_logits, dim=-1)\n",
        "\n",
        "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "            generated_ids.append(next_token_id)\n",
        "            if next_token_id == EOS_IDX:\n",
        "                break\n",
        "\n",
        "    generated_text = ' '.join(vocab.lookup_tokens(generated_ids))\n",
        "    generated_text = generated_text.replace(vocab.lookup_token(BOS_IDX), '')\n",
        "    generated_text = generated_text.replace(vocab.lookup_token(EOS_IDX), '')\n",
        "    generated_text = generated_text.replace(vocab.lookup_token(PAD_IDX), '')\n",
        "    return ' '.join(generated_text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23gWwRSMOo1n",
        "outputId": "a02f3dc5-953b-4ddf-b2fc-59a62e7735db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example text generation:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generation with the original (full precision) model\n",
        "start_phrase = \"This movie was\"\n",
        "generated_sentence = generate_text(model, vocab, start_phrase, max_len=30)\n",
        "print(f\"Prompt: '{start_phrase}'\\nGenerated (Original): '{generated_sentence}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-CkA6SrOout",
        "outputId": "edbe0755-167e-4711-bc1d-fa7a660cd41c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'This movie was'\n",
            "Generated (Original): 'this movie was that is not so this movie would be , and in the ' s girlfriend . i script is not if she are not bad . i was [1953/7] ('\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generation with the quantized model\n",
        "start_phrase_quant = \"I did not like\"\n",
        "generated_sentence_quant = generate_text(quantized_model, vocab, start_phrase_quant, max_len=30)\n",
        "print(f\"Prompt: '{start_phrase_quant}'\\nGenerated (Quantized): '{generated_sentence_quant}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQvid9geOor6",
        "outputId": "d0888e6f-50b0-44cd-94fc-192f7a05438f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'I did not like'\n",
            "Generated (Quantized): 'i did not like its the uckridge . i and i have ever appear , but the case , i have the young film ? it is not . it look the young e'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- After 'Evaluating quantized model...' print statement ---\n",
        "\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "\n",
        "# A. Evaluate Original Model\n",
        "print(\"Evaluating original model...\")\n",
        "original_loss, original_perplexity = evaluate(test_dataloader, model, criterion)\n",
        "print(f\"Original Model Test Loss: {original_loss:.4f}\")\n",
        "print(f\"Original Model Test Perplexity: {original_perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wK60xqFPN98",
        "outputId": "c20b20b3-fed8-42d4-b3bb-3b733c993bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Comparison ---\n",
            "Evaluating original model...\n",
            "Original Model Test Loss: 6.0714\n",
            "Original Model Test Perplexity: 433.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_BATCH_SIZE = 2\n",
        "\n",
        "test_dataloader = DataLoader(test_iter, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "z3N4gVzCdsq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Evaluate Quantized Model\n",
        "print(\"\\nEvaluating quantized model...\")\n",
        "quantized_loss, quantized_perplexity = evaluate(test_dataloader, quantized_model, criterion)\n",
        "print(f\"Quantized Model Test Loss: {quantized_loss:.4f}\")\n",
        "print(f\"Quantized Model Test Perplexity: {quantized_perplexity:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cmDjEqia2Te",
        "outputId": "2a98196b-9161-4472-bfaa-49b3e03b4b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating quantized model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C. Compare Model Sizes (in-memory)\n",
        "original_size_mb = sum(p.element_size() * p.numel() for p in model.parameters()) / (1024**2)\n",
        "quantized_size_mb = sum(p.element_size() * p.numel() for p in quantized_model.parameters()) / (1024**2)\n",
        "\n",
        "print(f\"\\nOriginal Model Size (in-memory): {original_size_mb:.2f} MB\")\n",
        "print(f\"Quantized Model Size (in-memory): {quantized_size_mb:.2f} MB\")\n",
        "print(f\"Memory Reduction: {((original_size_mb - quantized_size_mb) / original_size_mb) * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Iiu7Zwa1a2Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D. Compare Model Sizes (disk - optional, requires saving)\n",
        "# This requires saving the models to disk first:\n",
        "# torch.save(model.state_dict(), \"original_model.pth\")\n",
        "# torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
        "# original_disk_size_mb = os.path.getsize(\"original_model.pth\") / (1024**2)\n",
        "# quantized_disk_size_mb = os.path.getsize(\"quantized_model.pth\") / (1024**2)\n",
        "# print(f\"Original Model Size (on disk): {original_disk_size_mb:.2f} MB\")\n",
        "# print(f\"Quantized Model Size (on disk): {quantized_disk_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "id": "nWR-yXtLa2Ny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}